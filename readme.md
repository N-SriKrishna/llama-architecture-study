# ðŸ¦™ LLaMA Architecture Study  
A clean, modular TensorFlow implementation of the **LLaMA (Decoder-only Transformer)** architecture for text summarization.

---

## ðŸš€ Overview

This project is a **from-scratch educational implementation** of the LLaMA architecture (Meta AI), focused on readability and modular design.

It includes:

- âœ… Token & Positional Embeddings
- âœ… Multi-Head Self-Attention with **RoPE (Rotary Positional Encoding)**
- âœ… **RMSNorm** (Root Mean Square Normalization)
- âœ… **SwiGLU** Feed-Forward activation
- âœ… Full **Transformer Decoder** architecture

> The goal is to make LLaMA internals easy to understand and modify for experimentation.

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/N-SriKrishna/llama-architecture-study.git
cd llama-architecture-study
pip install -r requirements.txt
```

## Quick Start
```
python scripts/train.py
```

# Generate text
```
python scripts/generate.py
```

## Project Structure
```
llama-architecture-study/
â”œâ”€â”€ src/                      # Core architecture and components
â”‚   â”œâ”€â”€ layers/              # RMSNorm, SwiGLU, RoPE, Attention
â”‚   â”œâ”€â”€ model.py             # LLaMA decoder model
â”‚   â””â”€â”€ utils/               # Helpers (tokenizer, config loader, etc.)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # Training entry point
â”‚   â””â”€â”€ generate.py          # Inference / text generation
â”œâ”€â”€ configs/                 # Model & training config files (JSON)
â”œâ”€â”€ notebooks/               # Jupyter notebook version of the project
â””â”€â”€ README.md
```

## References

LLaMA Paper: https://arxiv.org/abs/2302.13971
Attention Is All You Need: https://arxiv.org/abs/1706.03762